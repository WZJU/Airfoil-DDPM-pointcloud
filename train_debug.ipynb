{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3258f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is training code of Airfoil_DDPM.\n",
    "# Author: Zhe Wen\n",
    "# Date: 2025-5-22\n",
    "# Copyright (c) Zhejiang University. All rights reserved.\n",
    "# See LICENSE file in the project root for license information.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from Airfoil_DDPM_pointcloud import Unet\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from mlp_data import MyData\n",
    "import pandas as pd\n",
    "#import h5py\n",
    "import csv\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import auxiliary.argument_parser as argument_parser\n",
    "import auxiliary.Dataloader as Dataloader\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89882ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_tensor(batch_size, dim, size):\n",
    "    tensor = torch.empty(batch_size, dim, size)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(dim):\n",
    "            tensor[i, j] = torch.normal(mean=0.0, std=1.0, size=(size,))\n",
    "    return tensor\n",
    "\n",
    "def partial_load_state_dict(model, checkpoint):\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = checkpoint['models']\n",
    "    filtered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}\n",
    "    model_dict.update(filtered_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "'''beta和alpha定义，用来混噪声合'''\n",
    "def cal_alpha_bar(alpha, max_t):\n",
    "    '''\n",
    "    alpha : torch[max_t]\n",
    "    sqrtalphabar, sqrt_1_m_alphabar : torch[max_t]\n",
    "    '''\n",
    "    sqrtalphabar=torch.empty(max_t)\n",
    "    sqrt_1_m_alphabar=torch.empty(max_t)\n",
    "    alphabar_temp=1\n",
    "    for i in range(max_t):\n",
    "        alphabar_temp=alphabar_temp*alpha[i]\n",
    "        sqrtalphabar[i]=sqrt(alphabar_temp)\n",
    "        sqrt_1_m_alphabar[i]=sqrt(1-alphabar_temp)\n",
    "    return sqrtalphabar, sqrt_1_m_alphabar\n",
    "\n",
    "alpha=1-torch.linspace(0.0001, 0.02, steps=1000)\n",
    "sqrtalphabar,sqrt_1_m_alphabar=cal_alpha_bar(alpha, 1000)\n",
    "\n",
    "\n",
    "def forward_propagation(model, loss, metric, input, context_1 = None, context_2 = None, time_step = 1000, device = 'cuda'):\n",
    "    '''\n",
    "    Input:\n",
    "        input: Tensor [B, N, C]\n",
    "        context : Tensor [(time_step-1)*B, 1, C] or None\n",
    "    return: loss, metric\n",
    "    '''\n",
    "    noise_tensor = torch.randn(time_step, input.size(0), input.size(1), input.size(2)) #[time_step, B, N, C]\n",
    "    input_tensor = input.unsqueeze(0) #[1,B,N,C]\n",
    "    input_tensor = input_tensor.expand(time_step, *input_tensor.shape[1:])#广播#[time_step,B,N,C]\n",
    "    # 将一维张量扩展到与 input_tensor 和 noise_tensor 相匹配的维度\n",
    "    sqrtalphabar_expanded = sqrtalphabar.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(input_tensor) #torch[time_step, 1 , 1, 1]->torch[time_step,B,N,C]\n",
    "    sqrt_1_m_alphabar_expanded = sqrt_1_m_alphabar.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(noise_tensor)#[time_step, B, N, C]\n",
    "    # 使用向量化操作进行计算\n",
    "    input_tensor = sqrtalphabar_expanded * input_tensor + sqrt_1_m_alphabar_expanded * noise_tensor #[time_step, B, N, C]\n",
    "    #处理反向预测数据\n",
    "    # 获取除了 t = 0 之外的切片\n",
    "    noise_tensor_no_t0 = noise_tensor[1:]#[time_step-1, B, N, C]\n",
    "    input_tensor_no_t0 = input_tensor[1:]#[time_step-1, B, N, C]\n",
    "    # 沿着第一个维度展开\n",
    "    noise_tensor_flattened = noise_tensor_no_t0.reshape(-1,noise_tensor_no_t0.shape[-2],noise_tensor_no_t0.shape[-1])#[(time_step-1)*B, N, C]\n",
    "    input_tensor_flattened = input_tensor_no_t0.reshape(-1,input_tensor_no_t0.shape[-2],input_tensor_no_t0.shape[-1])#[(time_step-1)*B, N, C]\n",
    "    # 获取 noise_tensor_flattened 的时间步总数\n",
    "    N = noise_tensor_no_t0.shape[0] #time_step-1\n",
    "    # 创建时间嵌入向量\n",
    "    time_embedding = torch.arange(1, N + 1, dtype=torch.float32).unsqueeze(-1).expand(N, noise_tensor_no_t0.shape[1]) #[time_step-1 , 1*B]\n",
    "    time_embedding = time_embedding.reshape(N*noise_tensor_no_t0.shape[1],-1) #[(time_step-1)*B , 1]\n",
    "\n",
    "    time_embedding=time_embedding.to(device)\n",
    "    input_tensor_flattened=input_tensor_flattened.to(device)\n",
    "    noise_tensor_flattened=noise_tensor_flattened.to(device)\n",
    "    print(input_tensor_flattened.shape)\n",
    "\n",
    "    pred_noise = model(input_tensor_flattened, time_embedding, context_1=context_1, context_2=context_2)\n",
    "    loss = loss(pred_noise, noise_tensor_flattened)\n",
    "    metric = metric(pred_noise, noise_tensor_flattened)\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_info(epoch, current_lr, loss, log_dir, type='step'):\n",
    "    if type=='step':\n",
    "        train_process=os.path.join(log_dir, 'step_info.csv')\n",
    "    elif type=='epoch':\n",
    "        train_process=os.path.join(log_dir, 'epoch_info.csv')\n",
    "\n",
    "    with open(train_process, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([epoch, current_lr, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd38bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ...\n"
     ]
    }
   ],
   "source": [
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "    \n",
    "    '''DEVICE'''\n",
    "    device = 'cuda'\n",
    "\n",
    "    '''CREATE DIR'''\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    exp_dir = os.path.join('log')\n",
    "    exp_dir = Path(exp_dir)\n",
    "    # exp_dir = Path('E:/wenzhe/generate_3/Encoder-Decoder/log/')\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    exp_dir = exp_dir.joinpath('pointcloud_diffusion')\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    exp_dir = exp_dir.joinpath(timestr)\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = exp_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = exp_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    '''LOG'''\n",
    "    #args = parse_args()\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, 'pointcloud_diffusion'))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    # log_string(opt)\n",
    "\n",
    "    train_process = os.path.join(log_dir, \"train_process.csv\")\n",
    "    valid_process = os.path.join(log_dir, \"valid_process.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''Init'''\n",
    "    df_model=Unet( num = [200, 100, 50, 100, 200], dim = [2, 2, 4, 2, 2], context_dim_1 = 3, context_dim_2 = 3, dropout = 0.).to(device)#\n",
    "    # checkpoint = torch.load('E:/wenzhe/generation2/airfoil_diffusion/models3/DFmodel_context_c3.1.5_100.pth', map_location=torch.device(device),weights_only=True)   ### 加载神经网络模型\n",
    "    # df_model = partial_load_state_dict(df_model, checkpoint)\n",
    "\n",
    "    loss = nn.MSELoss().to(device)\n",
    "    metric = nn.L1Loss().to(device)\n",
    "    optim = torch.optim.Adam(df_model.parameters(), lr=0.0001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=20, gamma=0.3, last_epoch=-1)\n",
    "    time_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''Load Data'''\n",
    "    data_path = os.path.join(\"../data/train_data_pointcloud.npz\")\n",
    "    data = np.load(data_path)\n",
    "    loaded_pointcloud = np.transpose(data['pointcloud'],(0,2,1))  # 形状 [B, N, D]-> [B, D, N]\n",
    "    loaded_ACC = data['ACC']                # 形状 [B, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''Dataloader'''\n",
    "    # 划分数据集，创建Dataset\n",
    "    len = loaded_pointcloud.shape[0]\n",
    "    train_len = int(len*0.8)\n",
    "    valid_len = int(len*0.1)\n",
    "    train_dataset = Dataloader.PointCloudACCDataset(loaded_pointcloud[:train_len], loaded_ACC[:train_len])\n",
    "    valid_dataset = Dataloader.PointCloudACCDataset(loaded_pointcloud[train_len:valid_len+train_len], loaded_ACC[train_len:valid_len+train_len])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,#这不用注释了吧\n",
    "        shuffle=True,# 打乱\n",
    "        num_workers=4,# 多进程加载数\n",
    "        pin_memory=True  # 加速GPU传输\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=32,#这不用注释了吧\n",
    "        shuffle=True,# 打乱\n",
    "        num_workers=4,# 多进程加载数\n",
    "        pin_memory=True  # 加速GPU传输\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2cf974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([999, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "    train_step=0\n",
    "    for batch in train_dataloader:\n",
    "        pointcloud_batch = batch['pointcloud'].permute(0,2,1)  # 形状 [batch_size, N, 3]\n",
    "        context_1 = batch['context_1']                # 形状 [batch_size, 3]\n",
    "        context_2 = batch['context_2']                # 形状 [batch_size, 3]\n",
    "\n",
    "        if context_1 is not None:\n",
    "            context_1 = context_1.unsqueeze(1).to(device) #[B, 1, C]\n",
    "            context_1 = context_1.unsqueeze(0) #[1, B, 1, C]\n",
    "            #广播到时间维度上\n",
    "            context_1 = context_1.expand(time_step-1, *context_1.shape[1:])#广播,但是少一次，因为t=0的时候不用 #[time_step-1, B, 1, C]\n",
    "            context_1 = context_1.reshape(-1,context_1.shape[-2],context_1.shape[-1]) #[(time_step-1)*B, 1, C]\n",
    "        \n",
    "        if context_2 is not None:\n",
    "            context_2 = context_2.unsqueeze(1).to(device)\n",
    "            context_2 = context_2.unsqueeze(0)\n",
    "            #广播到时间维度上\n",
    "            context_2 = context_2.expand(time_step-1, *context_2.shape[1:])#广播,但是少一次，因为t=0的时候不用\n",
    "            context_2 = context_2.reshape(-1,context_2.shape[-2],context_2.shape[-1])\n",
    "\n",
    "        #第一次正向传播和反向传播\n",
    "        step_loss, step_metric = forward_propagation(df_model, loss, metric, pointcloud_batch, context_1 = context_1, context_2 = context_2, time_step = time_step, device = 'cuda')\n",
    "        break\n",
    "            # optim.zero_grad()\n",
    "            # step_loss.backward()\n",
    "            # optim.step()\n",
    "            # train_step+=1\n",
    "            # current_lr = optim.param_groups[0]['lr']\n",
    "            # with open(train_process, 'a', newline='') as csvfile:\n",
    "            #     writer = csv.writer(csvfile)\n",
    "            #     writer.writerow([epoch + 1,train_step,current_lr, step_loss.item(), step_metric.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
