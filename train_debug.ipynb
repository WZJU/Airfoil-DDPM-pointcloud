{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3258f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is training code of Airfoil_DDPM.\n",
    "# Author: Zhe Wen\n",
    "# Date: 2025-5-22\n",
    "# Copyright (c) Zhejiang University. All rights reserved.\n",
    "# See LICENSE file in the project root for license information.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from Airfoil_DDPM_pointcloud import Unet\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from mlp_data import MyData\n",
    "import pandas as pd\n",
    "#import h5py\n",
    "import csv\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import auxiliary.argument_parser as argument_parser\n",
    "import auxiliary.Dataloader as Dataloader\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89882ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_tensor(batch_size, dim, size):\n",
    "    tensor = torch.empty(batch_size, dim, size)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(dim):\n",
    "            tensor[i, j] = torch.normal(mean=0.0, std=1.0, size=(size,))\n",
    "    return tensor\n",
    "\n",
    "def partial_load_state_dict(model, checkpoint):\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = checkpoint['models']\n",
    "    filtered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}\n",
    "    model_dict.update(filtered_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "'''beta和alpha定义，用来混噪声合'''\n",
    "def cal_alpha_bar(alpha, max_t):\n",
    "    '''\n",
    "    alpha : torch[max_t]\n",
    "    sqrtalphabar, sqrt_1_m_alphabar : torch[max_t]\n",
    "    '''\n",
    "    sqrtalphabar=torch.empty(max_t)\n",
    "    sqrt_1_m_alphabar=torch.empty(max_t)\n",
    "    alphabar_temp=1\n",
    "    for i in range(max_t):\n",
    "        alphabar_temp=alphabar_temp*alpha[i]\n",
    "        sqrtalphabar[i]=sqrt(alphabar_temp)\n",
    "        sqrt_1_m_alphabar[i]=sqrt(1-alphabar_temp)\n",
    "    return sqrtalphabar, sqrt_1_m_alphabar\n",
    "\n",
    "alpha=1-torch.linspace(0.0001, 0.02, steps=1000)\n",
    "sqrtalphabar,sqrt_1_m_alphabar=cal_alpha_bar(alpha, 1000)\n",
    "\n",
    "\n",
    "# def forward_propagation(model, loss, metric, input, context_1 = None, context_2 = None, time_step = 1000, device = 'cuda'):\n",
    "#     '''\n",
    "#     Input:\n",
    "#         input: Tensor [B, N, C]\n",
    "#         context : Tensor [(time_step-1)*B, 1, C] or None\n",
    "#     return: loss, metric\n",
    "#     '''\n",
    "#     noise_tensor = torch.randn(time_step, input.size(0), input.size(1), input.size(2)) #[time_step, B, N, C]\n",
    "#     input_tensor = input.unsqueeze(0) #[1,B,N,C]\n",
    "#     input_tensor = input_tensor.expand(time_step, *input_tensor.shape[1:])#广播#[time_step,B,N,C]\n",
    "#     # 将一维张量扩展到与 input_tensor 和 noise_tensor 相匹配的维度\n",
    "#     sqrtalphabar_expanded = sqrtalphabar.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(input_tensor) #torch[time_step, 1 , 1, 1]->torch[time_step,B,N,C]\n",
    "#     sqrt_1_m_alphabar_expanded = sqrt_1_m_alphabar.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(noise_tensor)#[time_step, B, N, C]\n",
    "#     # 使用向量化操作进行计算\n",
    "#     input_tensor = sqrtalphabar_expanded * input_tensor + sqrt_1_m_alphabar_expanded * noise_tensor #[time_step, B, N, C]\n",
    "#     #处理反向预测数据\n",
    "#     # 获取除了 t = 0 之外的切片\n",
    "#     noise_tensor_no_t0 = noise_tensor[1:]#[time_step-1, B, N, C]\n",
    "#     input_tensor_no_t0 = input_tensor[1:]#[time_step-1, B, N, C]\n",
    "#     # 沿着第一个维度展开\n",
    "#     noise_tensor_flattened = noise_tensor_no_t0.reshape(-1,noise_tensor_no_t0.shape[-2],noise_tensor_no_t0.shape[-1])#[(time_step-1)*B, N, C]\n",
    "#     input_tensor_flattened = input_tensor_no_t0.reshape(-1,input_tensor_no_t0.shape[-2],input_tensor_no_t0.shape[-1])#[(time_step-1)*B, N, C]\n",
    "#     # 获取 noise_tensor_flattened 的时间步总数\n",
    "#     N = noise_tensor_no_t0.shape[0] #time_step-1\n",
    "#     # 创建时间嵌入向量\n",
    "#     time_embedding = torch.arange(1, N + 1, dtype=torch.float32).unsqueeze(-1).expand(N, noise_tensor_no_t0.shape[1]) #[time_step-1 , 1*B]\n",
    "#     time_embedding = time_embedding.reshape(N*noise_tensor_no_t0.shape[1],-1) #[(time_step-1)*B , 1]\n",
    "\n",
    "#     time_embedding=time_embedding.to(device)\n",
    "#     input_tensor_flattened=input_tensor_flattened.to(device)\n",
    "#     noise_tensor_flattened=noise_tensor_flattened.to(device)\n",
    "\n",
    "#     print(input_tensor_flattened.shape)\n",
    "#     print(time_embedding.shape)\n",
    "\n",
    "#     pred_noise = model(input_tensor_flattened, time_embedding, context_1=context_1, context_2=context_2)\n",
    "#     loss = loss(pred_noise, noise_tensor_flattened)\n",
    "#     metric = metric(pred_noise, noise_tensor_flattened)\n",
    "#     return loss, metric\n",
    "\n",
    "def forward_propagation(model, loss, metric, input, context_1 = None, context_2 = None, time_step = 1000, device = 'cuda'):\n",
    "    '''\n",
    "    Input:\n",
    "        input: Tensor [B, D, N]\n",
    "        context : Tensor [(time_step-1)*B, 1, D(3)] or None\n",
    "    return: loss, metric\n",
    "    '''\n",
    "    noise_tensor = torch.randn(time_step, input.size(0), input.size(1), input.size(2)) #[time_step, B, D, N]\n",
    "    input_tensor = input.unsqueeze(0) #[1,B,N,C]\n",
    "    input_tensor = input_tensor.expand(time_step, *input_tensor.shape[1:])#广播#[time_step, B, D, N]\n",
    "    # 将一维张量扩展到与 input_tensor 和 noise_tensor 相匹配的维度\n",
    "    sqrtalphabar_expanded = sqrtalphabar.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(input_tensor) #torch[time_step, 1 , 1, 1]->torch[time_step, B, D, N]\n",
    "    sqrt_1_m_alphabar_expanded = sqrt_1_m_alphabar.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(noise_tensor)#[time_step, B, D, N]\n",
    "    # 使用向量化操作进行计算\n",
    "    input_tensor = sqrtalphabar_expanded * input_tensor + sqrt_1_m_alphabar_expanded * noise_tensor #[time_step, B, D, N]\n",
    "    #处理反向预测数据\n",
    "    # 获取除了 t = 0 之外的切片\n",
    "    noise_tensor_no_t0 = noise_tensor[1:]#[time_step-1, B, D, N]\n",
    "    input_tensor_no_t0 = input_tensor[1:]#[time_step-1, B, D, N]\n",
    "    # 沿着第一个维度展开\n",
    "    noise_tensor_flattened = noise_tensor_no_t0.reshape(-1,noise_tensor_no_t0.shape[-2],noise_tensor_no_t0.shape[-1])#[(time_step-1)*B, D, N]\n",
    "    input_tensor_flattened = input_tensor_no_t0.reshape(-1,input_tensor_no_t0.shape[-2],input_tensor_no_t0.shape[-1])#[(time_step-1)*B, D, N]\n",
    "    # 获取 noise_tensor_flattened 的时间步总数\n",
    "    N = noise_tensor_no_t0.shape[0] #time_step-1\n",
    "    # 创建时间嵌入向量\n",
    "    time_embedding = torch.arange(1, N + 1, dtype=torch.float32).unsqueeze(-1).expand(N, noise_tensor_no_t0.shape[1]) #[time_step-1 , 1*B]\n",
    "    time_embedding = time_embedding.reshape(N*noise_tensor_no_t0.shape[1],-1) #[(time_step-1)*B , 1]\n",
    "\n",
    "    time_embedding=time_embedding.to(device)\n",
    "    input_tensor_flattened=input_tensor_flattened.to(device)\n",
    "    noise_tensor_flattened=noise_tensor_flattened.to(device)\n",
    "\n",
    "    pred_noise_context = model(input_tensor_flattened, time_embedding, context_1=context_1, context_2=context_2)\n",
    "    loss = loss(pred_noise_context, noise_tensor_flattened)\n",
    "    metric = metric(pred_noise_context, noise_tensor_flattened)\n",
    "    print(input_tensor_flattened.shape)\n",
    "    print(time_embedding.shape)\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "def save_info(epoch, current_lr, loss, log_dir, type='step'):\n",
    "    if type=='step':\n",
    "        train_process=os.path.join(log_dir, 'step_info.csv')\n",
    "    elif type=='epoch':\n",
    "        train_process=os.path.join(log_dir, 'epoch_info.csv')\n",
    "\n",
    "    with open(train_process, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([epoch, current_lr, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd38bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ...\n"
     ]
    }
   ],
   "source": [
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "    \n",
    "    '''DEVICE'''\n",
    "    device = 'cuda'\n",
    "\n",
    "    '''CREATE DIR'''\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    exp_dir = os.path.join('log')\n",
    "    exp_dir = Path(exp_dir)\n",
    "    # exp_dir = Path('E:/wenzhe/generate_3/Encoder-Decoder/log/')\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    exp_dir = exp_dir.joinpath('pointcloud_diffusion')\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    exp_dir = exp_dir.joinpath(timestr)\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = exp_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = exp_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    '''LOG'''\n",
    "    #args = parse_args()\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, 'pointcloud_diffusion'))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    # log_string(opt)\n",
    "\n",
    "    train_process = os.path.join(log_dir, \"train_process.csv\")\n",
    "    valid_process = os.path.join(log_dir, \"valid_process.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4356fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''Init'''\n",
    "    df_model=Unet( num = [200, 50, 50, 50, 200], dim = [2, 2, 4, 2, 2], context_dim_1 = 3, context_dim_2 = 3, dropout = 0.).to(device)#\n",
    "    # checkpoint = torch.load('E:/wenzhe/generation2/airfoil_diffusion/models3/DFmodel_context_c3.1.5_100.pth', map_location=torch.device(device),weights_only=True)   ### 加载神经网络模型\n",
    "    # df_model = partial_load_state_dict(df_model, checkpoint)\n",
    "\n",
    "    loss = nn.MSELoss().to(device)\n",
    "    metric = nn.L1Loss().to(device)\n",
    "    optim = torch.optim.Adam(df_model.parameters(), lr=0.0001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=20, gamma=0.3, last_epoch=-1)\n",
    "    time_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aaf33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''Load Data'''\n",
    "    data_path = os.path.join(\"../data/train_data_pointcloud.npz\")\n",
    "    data = np.load(data_path)\n",
    "    loaded_pointcloud = np.transpose(data['pointcloud'],(0,2,1))  # 形状 [B, N, D]-> [B, D, N]\n",
    "    loaded_ACC = data['ACC']                # 形状 [B, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3044e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''Dataloader'''\n",
    "    # 划分数据集，创建Dataset\n",
    "    len = loaded_pointcloud.shape[0]\n",
    "    train_len = int(len*0.8)\n",
    "    valid_len = int(len*0.1)\n",
    "    train_dataset = Dataloader.PointCloudACCDataset(loaded_pointcloud[:train_len], loaded_ACC[:train_len])\n",
    "    valid_dataset = Dataloader.PointCloudACCDataset(loaded_pointcloud[train_len:valid_len+train_len], loaded_ACC[train_len:valid_len+train_len])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=10,#这不用注释了吧\n",
    "        shuffle=True,# 打乱\n",
    "        num_workers=2,# 多进程加载数\n",
    "        pin_memory=True  # 加速GPU传输\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=32,#这不用注释了吧\n",
    "        shuffle=True,# 打乱\n",
    "        num_workers=4,# 多进程加载数\n",
    "        pin_memory=True  # 加速GPU传输\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2cf974c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.51 GiB is allocated by PyTorch, and 3.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     context_2 \u001b[38;5;241m=\u001b[39m context_2\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,context_2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],context_2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#第一次正向传播和反向传播\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m step_loss, step_metric \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointcloud_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# optim.zero_grad()\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# step_loss.backward()\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# optim.step()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m#     writer = csv.writer(csvfile)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m#     writer.writerow([epoch + 1,train_step,current_lr, step_loss.item(), step_metric.item()])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 107\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(model, loss, metric, input, context_1, context_2, time_step, device)\u001b[0m\n\u001b[0;32m    104\u001b[0m input_tensor_flattened\u001b[38;5;241m=\u001b[39minput_tensor_flattened\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    105\u001b[0m noise_tensor_flattened\u001b[38;5;241m=\u001b[39mnoise_tensor_flattened\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 107\u001b[0m pred_noise_context \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss(pred_noise_context, noise_tensor_flattened)\n\u001b[0;32m    109\u001b[0m metric \u001b[38;5;241m=\u001b[39m metric(pred_noise_context, noise_tensor_flattened)\n",
      "File \u001b[1;32mc:\\Users\\WednZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WednZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\generate_2\\Airfoil-DDPM-pointcloud\\Airfoil_DDPM_pointcloud.py:224\u001b[0m, in \u001b[0;36mUnet.forward\u001b[1;34m(self, x, time_emb, context_1, context_2)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, time_emb, context_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, context_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    223\u001b[0m     d1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_down_1( x, time_emb, context \u001b[38;5;241m=\u001b[39m context_1)\n\u001b[1;32m--> 224\u001b[0m     d2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_down_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43md1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     u2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_up_2( d2, context \u001b[38;5;241m=\u001b[39m context_2, num_mod_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    226\u001b[0m     u1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_up_1( u2\u001b[38;5;241m+\u001b[39md1, context \u001b[38;5;241m=\u001b[39m context_2, num_mod_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\WednZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WednZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\generate_2\\Airfoil-DDPM-pointcloud\\Airfoil_DDPM_pointcloud.py:200\u001b[0m, in \u001b[0;36mUNet_Block.forward\u001b[1;34m(self, x, time_emb, context, num_mod_first)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_mod_first:\n\u001b[0;32m    199\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnummodify(x)\n\u001b[1;32m--> 200\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrossattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m num_mod_first:\n\u001b[0;32m    202\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnummodify(x)\n",
      "File \u001b[1;32mc:\\Users\\WednZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WednZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\generate_2\\Airfoil-DDPM-pointcloud\\Airfoil_DDPM_pointcloud.py:168\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[1;34m(self, x, context, mask)\u001b[0m\n\u001b[0;32m    164\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_k(x)\n\u001b[0;32m    166\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b (h n) d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mh), (q, k, v))\n\u001b[1;32m--> 168\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb i d, b j d -> b i j\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;66;03m#softmax(qk/sqrt(d))v中的qk/sqrt(d)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(mask):\n\u001b[0;32m    171\u001b[0m     mask \u001b[38;5;241m=\u001b[39m rearrange(mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ... -> b (...)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\WednZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.51 GiB is allocated by PyTorch, and 3.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "    train_step=0\n",
    "    for batch in train_dataloader:\n",
    "        pointcloud_batch = batch['pointcloud']  # 形状 [batch_size, N, 3]\n",
    "        context_1 = batch['context_1']                # 形状 [batch_size, 3]\n",
    "        context_2 = batch['context_2']                # 形状 [batch_size, 3]\n",
    "\n",
    "        if context_1 is not None:\n",
    "            context_1 = context_1.unsqueeze(1).to(device) #[B, 1, C]\n",
    "            context_1 = context_1.unsqueeze(0) #[1, B, 1, C]\n",
    "            #广播到时间维度上\n",
    "            context_1 = context_1.expand(time_step-1, *context_1.shape[1:])#广播,但是少一次，因为t=0的时候不用 #[time_step-1, B, 1, C]\n",
    "            context_1 = context_1.reshape(-1,context_1.shape[-2],context_1.shape[-1]) #[(time_step-1)*B, 1, C]\n",
    "        \n",
    "        if context_2 is not None:\n",
    "            context_2 = context_2.unsqueeze(1).to(device)\n",
    "            context_2 = context_2.unsqueeze(0)\n",
    "            #广播到时间维度上\n",
    "            context_2 = context_2.expand(time_step-1, *context_2.shape[1:])#广播,但是少一次，因为t=0的时候不用\n",
    "            context_2 = context_2.reshape(-1,context_2.shape[-2],context_2.shape[-1])\n",
    "\n",
    "        #第一次正向传播和反向传播\n",
    "        step_loss, step_metric = forward_propagation(df_model.eval(), loss, metric, pointcloud_batch, context_1 = context_1, context_2 = context_2, time_step = time_step, device = 'cuda')\n",
    "        break\n",
    "            # optim.zero_grad()\n",
    "            # step_loss.backward()\n",
    "            # optim.step()\n",
    "            # train_step+=1\n",
    "            # current_lr = optim.param_groups[0]['lr']\n",
    "            # with open(train_process, 'a', newline='') as csvfile:\n",
    "            #     writer = csv.writer(csvfile)\n",
    "            #     writer.writerow([epoch + 1,train_step,current_lr, step_loss.item(), step_metric.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
